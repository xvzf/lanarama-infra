{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lanarama Infrastructure & Server Setup Basic network topology The network topology sort of follows the spine-leaf concept. There are two switches for backbone routing and several access switches which connect to both of the backbone (further called core) switches. Infrastructure Edge Router / Firewall (WAN transfer net, BGP internal) KVM host which contains VMs running infrastructure and monitoring services (attached via BGP) Gameserver (tbd) Infrastructure Services & Monitoring DHCP Server (isc-dhcp-server) DNS Server (probably CoreDNS) tbd Prometheus for stats monitoring Elasticsearch for traffic monitoring/analysis","title":"Lanarama Infrastructure & Server Setup"},{"location":"#lanarama-infrastructure-server-setup","text":"","title":"Lanarama Infrastructure &amp; Server Setup"},{"location":"#basic-network-topology","text":"The network topology sort of follows the spine-leaf concept. There are two switches for backbone routing and several access switches which connect to both of the backbone (further called core) switches.","title":"Basic network topology"},{"location":"#infrastructure","text":"Edge Router / Firewall (WAN transfer net, BGP internal) KVM host which contains VMs running infrastructure and monitoring services (attached via BGP) Gameserver (tbd)","title":"Infrastructure"},{"location":"#infrastructure-services-monitoring","text":"DHCP Server (isc-dhcp-server) DNS Server (probably CoreDNS) tbd Prometheus for stats monitoring Elasticsearch for traffic monitoring/analysis","title":"Infrastructure Services &amp; Monitoring"},{"location":"fail/","text":"Fails (read this) DHCP has to run on a fucking physical interface and not a bridge . Don't add any kernel routes (e.g. with iproute 2) on the arista switches. It will break routing!","title":"Fails (read this)"},{"location":"fail/#fails-read-this","text":"DHCP has to run on a fucking physical interface and not a bridge . Don't add any kernel routes (e.g. with iproute 2) on the arista switches. It will break routing!","title":"Fails (read this)"},{"location":"network_bootstrap/","text":"Network Bootstrap The network configuration is fully automated and based on ansible . Provisioning core switches Currently there is only support for updating the config of the already reachable core switches. This will be adapted as soon as we have more than two backbone switches. Admins are lazy :-P Provisioning access switches Each access switch has to have the default cumulus linux login credentials configured. Additionally it has the IP address assigned to eth0 (the management interface) as stated in the provisioning inventory. Provisioning can be done by executing ansible-playbook -i inventory_provisioning access.yaml","title":"Network Bootstrap"},{"location":"network_bootstrap/#network-bootstrap","text":"The network configuration is fully automated and based on ansible .","title":"Network Bootstrap"},{"location":"network_bootstrap/#provisioning-core-switches","text":"Currently there is only support for updating the config of the already reachable core switches. This will be adapted as soon as we have more than two backbone switches. Admins are lazy :-P","title":"Provisioning core switches"},{"location":"network_bootstrap/#provisioning-access-switches","text":"Each access switch has to have the default cumulus linux login credentials configured. Additionally it has the IP address assigned to eth0 (the management interface) as stated in the provisioning inventory. Provisioning can be done by executing ansible-playbook -i inventory_provisioning access.yaml","title":"Provisioning access switches"},{"location":"network_connectivity/","text":"Network Connectivity Each access switch gets its own (private) IPv4 and (public) IPv6 subnet. NOTE IPv6 prefixes are added here as soon as we know what exactly we'll get! Access IPv4 Each access switch gets its own IPv4 subnet following this schema: 192.168.<access-id>.0/24 The .1 in the network is always the switch and will act as gateway for connecting clients IPv6 tbd IP address configuration For IPv6 we are using SLAAC - the IPv4 part is covered by a DHCP server running on kvm0 to which the access switches connect as a dhcp-relay. Servers heaven0 heaven0 is currently the only edge router. It NATs IPv4 and forwards IPv6 traffic. Additionally it does connection tracking in order to implement some basic network security for the gamers. There is no traffic shaping in place yet as we get a 10 Gigabit uplink connection which we probably will not saturate. kvm0 kvm0 is a virtual-machine host providing several instances like the DHCP server or monitoring. It advertises 192.168.122.0/24","title":"Network Connectivity"},{"location":"network_connectivity/#network-connectivity","text":"Each access switch gets its own (private) IPv4 and (public) IPv6 subnet. NOTE IPv6 prefixes are added here as soon as we know what exactly we'll get!","title":"Network Connectivity"},{"location":"network_connectivity/#access","text":"","title":"Access"},{"location":"network_connectivity/#ipv4","text":"Each access switch gets its own IPv4 subnet following this schema: 192.168.<access-id>.0/24 The .1 in the network is always the switch and will act as gateway for connecting clients","title":"IPv4"},{"location":"network_connectivity/#ipv6","text":"tbd","title":"IPv6"},{"location":"network_connectivity/#ip-address-configuration","text":"For IPv6 we are using SLAAC - the IPv4 part is covered by a DHCP server running on kvm0 to which the access switches connect as a dhcp-relay.","title":"IP address configuration"},{"location":"network_connectivity/#servers","text":"","title":"Servers"},{"location":"network_connectivity/#heaven0","text":"heaven0 is currently the only edge router. It NATs IPv4 and forwards IPv6 traffic. Additionally it does connection tracking in order to implement some basic network security for the gamers. There is no traffic shaping in place yet as we get a 10 Gigabit uplink connection which we probably will not saturate.","title":"heaven0"},{"location":"network_connectivity/#kvm0","text":"kvm0 is a virtual-machine host providing several instances like the DHCP server or monitoring. It advertises 192.168.122.0/24","title":"kvm0"},{"location":"network_debugging/","text":"Helpful tools/commands FRR We are using frrouting as a routing daemon on linux systems (more specific, advertising internal subnets via BGP). Frr is also configuring IP addresses of the interfaces. Privileged users can enter the configuration shell (similar to cisco) with: sudo vtysh","title":"Network debugging"},{"location":"network_debugging/#helpful-toolscommands","text":"","title":"Helpful tools/commands"},{"location":"network_debugging/#frr","text":"We are using frrouting as a routing daemon on linux systems (more specific, advertising internal subnets via BGP). Frr is also configuring IP addresses of the interfaces. Privileged users can enter the configuration shell (similar to cisco) with: sudo vtysh","title":"FRR"},{"location":"network_topo/","text":"Network Topology Network connectivity heavily relies on the BGP(4) protocol. Each access switch has its own AS number (detailed information following) and peers with both core switches which also have their own AS number. Hardware overview Core layer The two core switches consists of two Arista DCS7050SX-64 which peer via an 80 Gigabit interconnect. Ideally this link never sees much traffic and if so, something is going wrong with routing or there is a physical damage on the connection between one of the core switches and one of the access switches. Highspeed access layer tbd Uplink Layer Currently there is one Uplink gateway which is \"just\" a Dell R420 (2x E5-2430v2, 64GB DDR3) NATing with nftables and forwarding IPv6 traffic. It peers with the Core layer and advertises a default route. Service Layer Servers are also integrated here. Hardware varies. Access Layer We currently have 7 access switches planned ( Penguin Computing 4804i-q ) which have a 10 Gigabit connection to each of the core switches. ECMP ( E qual C ost M ultipath R outing) is enabled which ensures the switch routes packets over both link simultaniously. The switch performs a hash algorithm on the source/destination and decides on which link to send the packet. BGP internals We are using the private 16bit AS-range for our network. Core layer Core switches are running Aristas EOS. core0 -> AS64666 core1 -> AS64667 Uplink layer The uplink gateway is running frrouting and advertises a default route. heaven0 -> AS65001 Service layer Our servers are also running frrouting and advertises their internal network via BGP. kvm0 -> AS64801 Access layer The access switches are running the (seemingly acient) quagga routing suite. access1 -> AS64701 access2 -> AS64702 access3 -> AS64703 access4 -> AS64704 access5 -> AS64705 access6 -> AS64706 access7 -> AS64707 Connecting to the switches Every switch has a unique IP which is advertised and should be available from each point in the network. It is derived from the AS number. Some examples: - 64666 -> 10.64.66.6/32 - 64702 -> 10.64.70.2/32 - 65001 -> 10.65.0.1/32","title":"Network Topology"},{"location":"network_topo/#network-topology","text":"Network connectivity heavily relies on the BGP(4) protocol. Each access switch has its own AS number (detailed information following) and peers with both core switches which also have their own AS number.","title":"Network Topology"},{"location":"network_topo/#hardware-overview","text":"","title":"Hardware overview"},{"location":"network_topo/#core-layer","text":"The two core switches consists of two Arista DCS7050SX-64 which peer via an 80 Gigabit interconnect. Ideally this link never sees much traffic and if so, something is going wrong with routing or there is a physical damage on the connection between one of the core switches and one of the access switches.","title":"Core layer"},{"location":"network_topo/#highspeed-access-layer","text":"tbd","title":"Highspeed access layer"},{"location":"network_topo/#uplink-layer","text":"Currently there is one Uplink gateway which is \"just\" a Dell R420 (2x E5-2430v2, 64GB DDR3) NATing with nftables and forwarding IPv6 traffic. It peers with the Core layer and advertises a default route.","title":"Uplink Layer"},{"location":"network_topo/#service-layer","text":"Servers are also integrated here. Hardware varies.","title":"Service Layer"},{"location":"network_topo/#access-layer","text":"We currently have 7 access switches planned ( Penguin Computing 4804i-q ) which have a 10 Gigabit connection to each of the core switches. ECMP ( E qual C ost M ultipath R outing) is enabled which ensures the switch routes packets over both link simultaniously. The switch performs a hash algorithm on the source/destination and decides on which link to send the packet.","title":"Access Layer"},{"location":"network_topo/#bgp-internals","text":"We are using the private 16bit AS-range for our network.","title":"BGP internals"},{"location":"network_topo/#core-layer_1","text":"Core switches are running Aristas EOS. core0 -> AS64666 core1 -> AS64667","title":"Core layer"},{"location":"network_topo/#uplink-layer_1","text":"The uplink gateway is running frrouting and advertises a default route. heaven0 -> AS65001","title":"Uplink layer"},{"location":"network_topo/#service-layer_1","text":"Our servers are also running frrouting and advertises their internal network via BGP. kvm0 -> AS64801","title":"Service layer"},{"location":"network_topo/#access-layer_1","text":"The access switches are running the (seemingly acient) quagga routing suite. access1 -> AS64701 access2 -> AS64702 access3 -> AS64703 access4 -> AS64704 access5 -> AS64705 access6 -> AS64706 access7 -> AS64707","title":"Access layer"},{"location":"network_topo/#connecting-to-the-switches","text":"Every switch has a unique IP which is advertised and should be available from each point in the network. It is derived from the AS number. Some examples: - 64666 -> 10.64.66.6/32 - 64702 -> 10.64.70.2/32 - 65001 -> 10.65.0.1/32","title":"Connecting to the switches"}]}